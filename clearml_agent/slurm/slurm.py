from pathlib import Path
from typing import Any, OrderedDict

from clearml import Task
from omegaconf import DictConfig, OmegaConf

from clearml_agent.commands import Worker
from clearml_agent.helper.process import Argv
import os


class SlurmIntegration(Worker):
    SLURM_PENDING_QUEUE_NAME = 'slurm_pending'

    def run_one_task(
            self,
            queue: str,
            task_id: str,
            worker_args,
            docker=None,
            task_session=None
    ):

        # start new process and execute task id
        # "Running task '{}'".format(task_id)
        print(self._task_logging_start_message.format(task_id))
        task_session = task_session or self._session
        # set task status to in_progress so we know it was popped from the queue
        # noinspection PyBroadException

        log_dir = Path(os.path.expandvars(self._session.config.get('sdk.storage.log_dir'))).expanduser() / task_id
        log_dir.mkdir(parents=True, exist_ok=True)

        task = Task.get_task(task_id=task_id)
        cluster_cfg = self._to_omega_conf(task._get_configuration_dict('cluster_cfg'))
        cluster_cfg.log_dir = log_dir.as_posix()
        cluster_cfg.slurm_log_dir = str(Path(task_session.config.get('sdk.storage.log_dir')) / 'slurm_logs')
        cluster_cfg.job_name = task.name

        sbatch_file = self._gen_sbatch_options(task, cluster_cfg)

        sbatch_file += (
            f'srun python {os.getcwd()}/main.py execute --id {task_id} --full-monitoring'
        )

        sbatch_path = Path(cluster_cfg.slurm_log_dir) / f'{cluster_cfg.job_name}_run.sh'
        sbatch_path.parent.mkdir(exist_ok=True, parents=True)
        with open(sbatch_path, 'w') as f:
            f.write(sbatch_file)

        execute_command_file = str(log_dir / 'agent_log.out')
        status, stop_signal_status = self._log_command_output(
            task_id=task_id,
            cmd=Argv('sbatch', str(sbatch_path.absolute())),
            session=task_session,
            stdout_path=execute_command_file
        )

        if status == 0:
            # push task into the slurm queue, so we have visibility on pending tasks in slurm job list
            try:
                print('Pushing task {} into slurm pending queue'.format(task_id))
                res = self._session.api_client.tasks.stop(task_id, force=True)
                Task.enqueue(
                    task=task_id,
                    queue_name=self.SLURM_PENDING_QUEUE_NAME
                )
                if res.meta.result_code != 200:
                    raise Exception(res.meta.result_msg)
            except Exception as e:
                err = "ERROR: Could not push back task [{}] to slurm pending queue [{}], error: {}".format(
                    task_id, self.SLURM_PENDING_QUEUE_NAME, e)
                self.send_logs(task_id, [err], level='ERROR')
                self.log.error(err)
                return
        else:
            with open(execute_command_file, 'r') as f:
                self.send_logs(task_id, f.readlines(), level='INFO')

    def _gen_sbatch_options(
            self,
            task: Task,
            cfg: DictConfig
    ) -> str:
        sbatch_file_str = (
            '#!/bin/bash\n'
            '# Autogenerated by ClearML SLURM integration.\n\n'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='job-name',
            opt_value=cfg.job_name,
            description='Slurm job name.'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='output',
            opt_value=Path(cfg.slurm_log_dir) / 'slurm_out_log.out',
            description='Slurm out log file.'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='error',
            opt_value=Path(cfg.slurm_log_dir) / 'slurm_err_log.err',
            description='Slurm err log file.'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='time',
            opt_value=cfg.time,
            description='Job walltime.'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='gres',
            opt_value=f'gpu:{cfg.gpus_per_node}',
            description='GPUs per node.'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='cpus-per-task',
            opt_value=f'{cfg.cpus_per_node}',
            description='CPUs per node.'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='mem',
            opt_value=f'{cfg.mem_per_node * 1024}',
            description='Requested nodes.'
        )
        sbatch_file_str += self._gen_sbatch_option(
            opt_name='signal',
            opt_value=f'USR1@60',
            description='Signal that will be send before killing a job.'
        )

        sbatch_file_str += (
            '#\n#\n# Cluster specific options.\n#\n#\n\n'
        )

        if cfg.cluster_specific is not None:
            for n, v in cfg.cluster_specific.items():
                sbatch_file_str += self._gen_sbatch_option(
                    opt_name=n,
                    opt_value=v
                )
        return sbatch_file_str

    def _gen_sbatch_option(
            self,
            opt_name: str,
            opt_value: Any,
            description: str = ''
    ) -> str:
        return (
            f'#\n# {description}\n#\n'
            f'#SBATCH --{opt_name}={opt_value}\n'
            f'#######################\n\n'
        )

    def _to_omega_conf(
            self,
            cfg: OrderedDict
    ) -> DictConfig:

        def convert(c: OrderedDict) -> dict:
            d = {}
            for k, v in c.items():
                if isinstance(v, OrderedDict):
                    d[k] = convert(v)
                else:
                    d[k] = v
            return d

        return OmegaConf.create(convert(cfg))
